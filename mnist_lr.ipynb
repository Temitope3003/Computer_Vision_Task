{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have only used a fixed learning rate, but in practice, when working with larger datasets where you need to run about 200 to 300 epochs. Using a single learning rate would stop your accuracy from improving beyond certain points, the \n",
    "key is to gradually reduce your accuracy after certain number of epochs. If you started with a learning rate of 0.1, you might divide the learning rate by 10 after \n",
    "30 epochs, 60 epochs and 90 epochs, hence at epoch 90, you would have much lower learning rate. Keras provides a handy Learning Rate Scheduler to do this.\n",
    "\n",
    "Below is our first example, modified to use a dynamic learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x, train_y), (test_x, test_y) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the data\n",
    "train_x = train_x.astype('float32')/255\n",
    "test_x = test_x.astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten the image\n",
    "train_x = train_x.reshape(60000, 784)\n",
    "test_x = test_x.reshape(10000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode the labels to vectors\n",
    "train_y = keras.utils.to_categorical(train_y, 10)\n",
    "test_y = keras.utils.to_categorical(test_y, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units= 128, activation= 'relu', input_shape = (784,)))\n",
    "model.add(Dense(units = 128, activation= 'relu'))\n",
    "model.add(Dense(units = 128, activation= 'relu'))\n",
    "model.add(Dense(units = 128, activation= 'relu'))\n",
    "model.add(Dense(units = 128, activation= 'relu'))\n",
    "model.add(Dense(units = 128, activation= 'relu'))\n",
    "model.add(Dense(units = 128, activation= 'relu'))\n",
    "model.add(Dense(units = 10, activation= 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_11 (Dense)            (None, 128)               100480    \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 200842 (784.54 KB)\n",
      "Trainable params: 200842 (784.54 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the learning rate schedule function\n",
    "def lr_schedule(epoch):\n",
    "  lr = 0.1\n",
    "\n",
    "  if epoch > 15:\n",
    "    lr = lr/100\n",
    "\n",
    "  elif epoch > 10:\n",
    "    lr = lr/10\n",
    "\n",
    "  elif epoch >5:\n",
    "    lr = lr / 5\n",
    "\n",
    "  print ('Learning Rate: ', lr)\n",
    "\n",
    "  return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pass the scheduler function to the learning rate scheduler class\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate:  0.1\n"
     ]
    }
   ],
   "source": [
    "#specify the training components\n",
    "model.compile(optimizer= SGD(lr_schedule(0)), loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate:  0.1\n",
      "Epoch 1/30\n",
      "938/938 [==============================] - 4s 3ms/step - loss: 0.4782 - accuracy: 0.8464 - lr: 0.1000\n",
      "Learning Rate:  0.1\n",
      "Epoch 2/30\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.1397 - accuracy: 0.9574 - lr: 0.1000\n",
      "Learning Rate:  0.1\n",
      "Epoch 3/30\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0958 - accuracy: 0.9714 - lr: 0.1000\n",
      "Learning Rate:  0.1\n",
      "Epoch 4/30\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0730 - accuracy: 0.9780 - lr: 0.1000\n",
      "Learning Rate:  0.1\n",
      "Epoch 5/30\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0587 - accuracy: 0.9825 - lr: 0.1000\n",
      "Learning Rate:  0.1\n",
      "Epoch 6/30\n",
      "938/938 [==============================] - 3s 4ms/step - loss: 0.0477 - accuracy: 0.9855 - lr: 0.1000\n",
      "Learning Rate:  0.02\n",
      "Epoch 7/30\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0227 - accuracy: 0.9934 - lr: 0.0200\n",
      "Learning Rate:  0.02\n",
      "Epoch 8/30\n",
      "938/938 [==============================] - 4s 5ms/step - loss: 0.0162 - accuracy: 0.9957 - lr: 0.0200\n",
      "Learning Rate:  0.02\n",
      "Epoch 9/30\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0129 - accuracy: 0.9967 - lr: 0.0200\n",
      "Learning Rate:  0.02\n",
      "Epoch 10/30\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0109 - accuracy: 0.9973 - lr: 0.0200\n",
      "Learning Rate:  0.02\n",
      "Epoch 11/30\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0093 - accuracy: 0.9980 - lr: 0.0200\n",
      "Learning Rate:  0.01\n",
      "Epoch 12/30\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0074 - accuracy: 0.9985 - lr: 0.0100\n",
      "Learning Rate:  0.01\n",
      "Epoch 13/30\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.0067 - accuracy: 0.9987 - lr: 0.0100\n",
      "Learning Rate:  0.01\n",
      "Epoch 14/30\n",
      "938/938 [==============================] - 4s 5ms/step - loss: 0.0062 - accuracy: 0.9988 - lr: 0.0100\n",
      "Learning Rate:  0.01\n",
      "Epoch 15/30\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.0058 - accuracy: 0.9990 - lr: 0.0100\n",
      "Learning Rate:  0.01\n",
      "Epoch 16/30\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.0053 - accuracy: 0.9991 - lr: 0.0100\n",
      "Learning Rate:  0.001\n",
      "Epoch 17/30\n",
      "938/938 [==============================] - 4s 5ms/step - loss: 0.0047 - accuracy: 0.9992 - lr: 0.0010\n",
      "Learning Rate:  0.001\n",
      "Epoch 18/30\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0046 - accuracy: 0.9992 - lr: 0.0010\n",
      "Learning Rate:  0.001\n",
      "Epoch 19/30\n",
      "938/938 [==============================] - 4s 5ms/step - loss: 0.0046 - accuracy: 0.9993 - lr: 0.0010\n",
      "Learning Rate:  0.001\n",
      "Epoch 20/30\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0046 - accuracy: 0.9993 - lr: 0.0010\n",
      "Learning Rate:  0.001\n",
      "Epoch 21/30\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0045 - accuracy: 0.9992 - lr: 0.0010\n",
      "Learning Rate:  0.001\n",
      "Epoch 22/30\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0045 - accuracy: 0.9993 - lr: 0.0010\n",
      "Learning Rate:  0.001\n",
      "Epoch 23/30\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.0045 - accuracy: 0.9993 - lr: 0.0010\n",
      "Learning Rate:  0.001\n",
      "Epoch 24/30\n",
      "938/938 [==============================] - 4s 5ms/step - loss: 0.0044 - accuracy: 0.9993 - lr: 0.0010\n",
      "Learning Rate:  0.001\n",
      "Epoch 25/30\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0044 - accuracy: 0.9993 - lr: 0.0010\n",
      "Learning Rate:  0.001\n",
      "Epoch 26/30\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0044 - accuracy: 0.9993 - lr: 0.0010\n",
      "Learning Rate:  0.001\n",
      "Epoch 27/30\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0043 - accuracy: 0.9993 - lr: 0.0010\n",
      "Learning Rate:  0.001\n",
      "Epoch 28/30\n",
      "938/938 [==============================] - 4s 5ms/step - loss: 0.0043 - accuracy: 0.9993 - lr: 0.0010\n",
      "Learning Rate:  0.001\n",
      "Epoch 29/30\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.0043 - accuracy: 0.9993 - lr: 0.0010\n",
      "Learning Rate:  0.001\n",
      "Epoch 30/30\n",
      "938/938 [==============================] - 4s 5ms/step - loss: 0.0042 - accuracy: 0.9994 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x236040b81d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y, batch_size=  64, epochs= 30, verbose= 1, callbacks= [lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 2ms/step - loss: 0.0730 - accuracy: 0.9818\n",
      "Accuracy:  [0.0730333924293518, 0.9818000197410583]\n"
     ]
    }
   ],
   "source": [
    "accuracy = model.evaluate(x=test_x, y = test_y, batch_size= 64)\n",
    "print('Accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL CHECKPOINTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our examples so far, we only save the model after complete training. However, in practice, you would want to save your model after very N epochs. The reason is  that sometimes, our final epoch maybe less accurate than some epochs, usually we want the best, so by saving many, we can go back to a previously saved model that is better than our final model.\n",
    "\n",
    "Keras provides the ModelCheckpoint utility to handle this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the mnist dataset\n",
    "(train_x, train_y) , (test_x, test_y) = mnist.load_data()\n",
    "#normalize the data\n",
    "train_x = train_x.astype('float32') / 255\n",
    "test_x = test_x.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_19 (Dense)            (None, 128)               100480    \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134794 (526.54 KB)\n",
      "Trainable params: 134794 (526.54 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Flatten the images\n",
    "train_x = train_x.reshape(60000,784)\n",
    "test_x = test_x.reshape(10000,784)\n",
    "\n",
    "\n",
    "#Encode the labels to vectors\n",
    "train_y = keras.utils.to_categorical(train_y,10)\n",
    "test_y = keras.utils.to_categorical(test_y,10)\n",
    "\n",
    "\n",
    "#Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(units=128,activation=\"relu\",input_shape=(784,)))\n",
    "model.add(Dense(units=128,activation=\"relu\"))\n",
    "model.add(Dense(units=128,activation=\"relu\"))\n",
    "model.add(Dense(units=10,activation=\"softmax\"))\n",
    "\n",
    "\n",
    "#Print a Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the learning rate schedule function\n",
    "def lr_schedule(epoch):\n",
    "  lr = 0.1\n",
    "\n",
    "  if epoch > 15:\n",
    "    lr = lr/100\n",
    "\n",
    "  elif epoch > 10:\n",
    "    lr = lr/10\n",
    "\n",
    "  elif epoch >5:\n",
    "    lr = lr / 5\n",
    "\n",
    "  print ('Learning Rate: ', lr)\n",
    "\n",
    "  return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass the scheduler function to the Learning Rate Scheduler class\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "#Directory in which to create models\n",
    "save_direc = os.path.join(os.getcwd(), 'mnistsavedmodels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name of model files\n",
    "model_name = 'mnistmodel. {epoch:03d}.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a directory if it doesn't exist\n",
    "if not os.path.isdir(save_direc):\n",
    "  os.makedirs(save_direc)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join the directory with the model file\n",
    "modelpath = os.path.join(save_direc, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath=modelpath,\n",
    "                             monitor='val_accuracy',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True,\n",
    "                             mode='max',\n",
    "                             save_freq='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate:  0.1\n",
      "Learning Rate:  0.1\n",
      "Epoch 1/20\n",
      "1853/1875 [============================>.] - ETA: 0s - loss: 0.0103 - accuracy: 0.9966\n",
      "Epoch 1: val_accuracy improved from -inf to 0.97550, saving model to c:\\Users\\PC\\Desktop\\deep_learning\\mnistsavedmodels\\mnistmodel. 001.h5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0104 - accuracy: 0.9965 - val_loss: 0.1100 - val_accuracy: 0.9755 - lr: 0.1000\n",
      "Learning Rate:  0.1\n",
      "Epoch 2/20\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0098 - accuracy: 0.9967\n",
      "Epoch 2: val_accuracy improved from 0.97550 to 0.97860, saving model to c:\\Users\\PC\\Desktop\\deep_learning\\mnistsavedmodels\\mnistmodel. 002.h5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0098 - accuracy: 0.9967 - val_loss: 0.0866 - val_accuracy: 0.9786 - lr: 0.1000\n",
      "Learning Rate:  0.1\n",
      "Epoch 3/20\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0074 - accuracy: 0.9977\n",
      "Epoch 3: val_accuracy improved from 0.97860 to 0.97980, saving model to c:\\Users\\PC\\Desktop\\deep_learning\\mnistsavedmodels\\mnistmodel. 003.h5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0074 - accuracy: 0.9977 - val_loss: 0.0826 - val_accuracy: 0.9798 - lr: 0.1000\n",
      "Learning Rate:  0.1\n",
      "Epoch 4/20\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0068 - accuracy: 0.9979\n",
      "Epoch 4: val_accuracy improved from 0.97980 to 0.98100, saving model to c:\\Users\\PC\\Desktop\\deep_learning\\mnistsavedmodels\\mnistmodel. 004.h5\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0068 - accuracy: 0.9979 - val_loss: 0.0837 - val_accuracy: 0.9810 - lr: 0.1000\n",
      "Learning Rate:  0.1\n",
      "Epoch 5/20\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9990\n",
      "Epoch 5: val_accuracy did not improve from 0.98100\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0037 - accuracy: 0.9990 - val_loss: 0.0938 - val_accuracy: 0.9796 - lr: 0.1000\n",
      "Learning Rate:  0.1\n",
      "Epoch 6/20\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9988\n",
      "Epoch 6: val_accuracy improved from 0.98100 to 0.98140, saving model to c:\\Users\\PC\\Desktop\\deep_learning\\mnistsavedmodels\\mnistmodel. 006.h5\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.0038 - accuracy: 0.9988 - val_loss: 0.0932 - val_accuracy: 0.9814 - lr: 0.1000\n",
      "Learning Rate:  0.02\n",
      "Epoch 7/20\n",
      "1865/1875 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 7: val_accuracy improved from 0.98140 to 0.98220, saving model to c:\\Users\\PC\\Desktop\\deep_learning\\mnistsavedmodels\\mnistmodel. 007.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.0848 - val_accuracy: 0.9822 - lr: 0.0200\n",
      "Learning Rate:  0.02\n",
      "Epoch 8/20\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 4.1564e-04 - accuracy: 1.0000\n",
      "Epoch 8: val_accuracy improved from 0.98220 to 0.98300, saving model to c:\\Users\\PC\\Desktop\\deep_learning\\mnistsavedmodels\\mnistmodel. 008.h5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 4.1509e-04 - accuracy: 1.0000 - val_loss: 0.0854 - val_accuracy: 0.9830 - lr: 0.0200\n",
      "Learning Rate:  0.02\n",
      "Epoch 9/20\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 3.3638e-04 - accuracy: 1.0000\n",
      "Epoch 9: val_accuracy improved from 0.98300 to 0.98350, saving model to c:\\Users\\PC\\Desktop\\deep_learning\\mnistsavedmodels\\mnistmodel. 009.h5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 3.3638e-04 - accuracy: 1.0000 - val_loss: 0.0856 - val_accuracy: 0.9835 - lr: 0.0200\n",
      "Learning Rate:  0.02\n",
      "Epoch 10/20\n",
      "1858/1875 [============================>.] - ETA: 0s - loss: 2.9462e-04 - accuracy: 1.0000\n",
      "Epoch 10: val_accuracy did not improve from 0.98350\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 2.9442e-04 - accuracy: 1.0000 - val_loss: 0.0863 - val_accuracy: 0.9833 - lr: 0.0200\n",
      "Learning Rate:  0.02\n",
      "Epoch 11/20\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 2.6848e-04 - accuracy: 1.0000\n",
      "Epoch 11: val_accuracy did not improve from 0.98350\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 2.6832e-04 - accuracy: 1.0000 - val_loss: 0.0863 - val_accuracy: 0.9835 - lr: 0.0200\n",
      "Learning Rate:  0.01\n",
      "Epoch 12/20\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 2.4523e-04 - accuracy: 1.0000\n",
      "Epoch 12: val_accuracy did not improve from 0.98350\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 2.4597e-04 - accuracy: 1.0000 - val_loss: 0.0865 - val_accuracy: 0.9833 - lr: 0.0100\n",
      "Learning Rate:  0.01\n",
      "Epoch 13/20\n",
      "1858/1875 [============================>.] - ETA: 0s - loss: 2.3658e-04 - accuracy: 1.0000\n",
      "Epoch 13: val_accuracy did not improve from 0.98350\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 2.3696e-04 - accuracy: 1.0000 - val_loss: 0.0867 - val_accuracy: 0.9833 - lr: 0.0100\n",
      "Learning Rate:  0.01\n",
      "Epoch 14/20\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 2.2897e-04 - accuracy: 1.0000\n",
      "Epoch 14: val_accuracy did not improve from 0.98350\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 2.2881e-04 - accuracy: 1.0000 - val_loss: 0.0869 - val_accuracy: 0.9831 - lr: 0.0100\n",
      "Learning Rate:  0.01\n",
      "Epoch 15/20\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 2.2178e-04 - accuracy: 1.0000\n",
      "Epoch 15: val_accuracy did not improve from 0.98350\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 2.2140e-04 - accuracy: 1.0000 - val_loss: 0.0871 - val_accuracy: 0.9833 - lr: 0.0100\n",
      "Learning Rate:  0.01\n",
      "Epoch 16/20\n",
      "1862/1875 [============================>.] - ETA: 0s - loss: 2.1501e-04 - accuracy: 1.0000\n",
      "Epoch 16: val_accuracy did not improve from 0.98350\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 2.1469e-04 - accuracy: 1.0000 - val_loss: 0.0872 - val_accuracy: 0.9833 - lr: 0.0100\n",
      "Learning Rate:  0.001\n",
      "Epoch 17/20\n",
      "1860/1875 [============================>.] - ETA: 0s - loss: 2.0754e-04 - accuracy: 1.0000\n",
      "Epoch 17: val_accuracy did not improve from 0.98350\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 2.0775e-04 - accuracy: 1.0000 - val_loss: 0.0872 - val_accuracy: 0.9833 - lr: 0.0010\n",
      "Learning Rate:  0.001\n",
      "Epoch 18/20\n",
      "1863/1875 [============================>.] - ETA: 0s - loss: 2.0715e-04 - accuracy: 1.0000\n",
      "Epoch 18: val_accuracy did not improve from 0.98350\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 2.0715e-04 - accuracy: 1.0000 - val_loss: 0.0872 - val_accuracy: 0.9833 - lr: 0.0010\n",
      "Learning Rate:  0.001\n",
      "Epoch 19/20\n",
      "1864/1875 [============================>.] - ETA: 0s - loss: 2.0565e-04 - accuracy: 1.0000\n",
      "Epoch 19: val_accuracy did not improve from 0.98350\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 2.0657e-04 - accuracy: 1.0000 - val_loss: 0.0872 - val_accuracy: 0.9833 - lr: 0.0010\n",
      "Learning Rate:  0.001\n",
      "Epoch 20/20\n",
      "1860/1875 [============================>.] - ETA: 0s - loss: 2.0619e-04 - accuracy: 1.0000\n",
      "Epoch 20: val_accuracy did not improve from 0.98350\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 2.0599e-04 - accuracy: 1.0000 - val_loss: 0.0872 - val_accuracy: 0.9833 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x23607abb890>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Specify the training components\n",
    "model.compile(optimizer=SGD(lr_schedule(0)),loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "#Fit the model\n",
    "model.fit(train_x,train_y,batch_size=32,epochs=20,shuffle=True,verbose=1,validation_data=(test_x, test_y),callbacks=[checkpoint, lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate:  0.1\n",
      "Epoch 1/20\n",
      "1680/1688 [============================>.] - ETA: 0s - loss: 2.0179e-04 - accuracy: 1.0000\n",
      "Epoch 1: val_accuracy improved from 0.98350 to 1.00000, saving model to c:\\Users\\PC\\Desktop\\deep_learning\\mnistsavedmodels\\mnistmodel. 001.h5\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 2.0172e-04 - accuracy: 1.0000 - val_loss: 3.0519e-04 - val_accuracy: 1.0000 - lr: 0.1000\n",
      "Learning Rate:  0.1\n",
      "Epoch 2/20\n",
      "1673/1688 [============================>.] - ETA: 0s - loss: 1.6193e-04 - accuracy: 1.0000\n",
      "Epoch 2: val_accuracy did not improve from 1.00000\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 1.6157e-04 - accuracy: 1.0000 - val_loss: 3.6318e-04 - val_accuracy: 0.9998 - lr: 0.1000\n",
      "Learning Rate:  0.1\n",
      "Epoch 3/20\n",
      "1670/1688 [============================>.] - ETA: 0s - loss: 1.3362e-04 - accuracy: 1.0000\n",
      "Epoch 3: val_accuracy did not improve from 1.00000\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 1.3350e-04 - accuracy: 1.0000 - val_loss: 3.9737e-04 - val_accuracy: 0.9998 - lr: 0.1000\n",
      "Learning Rate:  0.1\n",
      "Epoch 4/20\n",
      "1679/1688 [============================>.] - ETA: 0s - loss: 1.1716e-04 - accuracy: 1.0000\n",
      "Epoch 4: val_accuracy did not improve from 1.00000\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 1.1731e-04 - accuracy: 1.0000 - val_loss: 4.2876e-04 - val_accuracy: 0.9998 - lr: 0.1000\n",
      "Learning Rate:  0.1\n",
      "Epoch 5/20\n",
      "1678/1688 [============================>.] - ETA: 0s - loss: 1.0399e-04 - accuracy: 1.0000\n",
      "Epoch 5: val_accuracy did not improve from 1.00000\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 1.0374e-04 - accuracy: 1.0000 - val_loss: 4.3551e-04 - val_accuracy: 0.9998 - lr: 0.1000\n",
      "Learning Rate:  0.1\n",
      "Epoch 6/20\n",
      "1687/1688 [============================>.] - ETA: 0s - loss: 9.4250e-05 - accuracy: 1.0000\n",
      "Epoch 6: val_accuracy did not improve from 1.00000\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 9.4232e-05 - accuracy: 1.0000 - val_loss: 4.6315e-04 - val_accuracy: 0.9998 - lr: 0.1000\n",
      "Learning Rate:  0.02\n",
      "Epoch 7/20\n",
      "1688/1688 [==============================] - ETA: 0s - loss: 8.5241e-05 - accuracy: 1.0000\n",
      "Epoch 7: val_accuracy did not improve from 1.00000\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 8.5241e-05 - accuracy: 1.0000 - val_loss: 4.7270e-04 - val_accuracy: 0.9998 - lr: 0.0200\n",
      "Learning Rate:  0.02\n",
      "Epoch 8/20\n",
      "1681/1688 [============================>.] - ETA: 0s - loss: 8.3510e-05 - accuracy: 1.0000\n",
      "Epoch 8: val_accuracy did not improve from 1.00000\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 8.3341e-05 - accuracy: 1.0000 - val_loss: 4.7908e-04 - val_accuracy: 0.9998 - lr: 0.0200\n",
      "Learning Rate:  0.02\n",
      "Epoch 9/20\n",
      "1687/1688 [============================>.] - ETA: 0s - loss: 8.1992e-05 - accuracy: 1.0000\n",
      "Epoch 9: val_accuracy did not improve from 1.00000\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 8.2013e-05 - accuracy: 1.0000 - val_loss: 4.8424e-04 - val_accuracy: 0.9998 - lr: 0.0200\n",
      "Learning Rate:  0.02\n",
      "Epoch 10/20\n",
      "1684/1688 [============================>.] - ETA: 0s - loss: 8.0849e-05 - accuracy: 1.0000\n",
      "Epoch 10: val_accuracy did not improve from 1.00000\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 8.0791e-05 - accuracy: 1.0000 - val_loss: 4.8928e-04 - val_accuracy: 0.9998 - lr: 0.0200\n",
      "Learning Rate:  0.02\n",
      "Epoch 11/20\n",
      "1688/1688 [==============================] - ETA: 0s - loss: 7.9585e-05 - accuracy: 1.0000\n",
      "Epoch 11: val_accuracy did not improve from 1.00000\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 7.9585e-05 - accuracy: 1.0000 - val_loss: 4.9450e-04 - val_accuracy: 0.9998 - lr: 0.0200\n",
      "Learning Rate:  0.01\n",
      "Epoch 12/20\n",
      "1683/1688 [============================>.] - ETA: 0s - loss: 7.8161e-05 - accuracy: 1.0000\n",
      "Epoch 12: val_accuracy did not improve from 1.00000\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 7.8247e-05 - accuracy: 1.0000 - val_loss: 4.9700e-04 - val_accuracy: 0.9998 - lr: 0.0100\n",
      "Learning Rate:  0.01\n",
      "Epoch 13/20\n",
      "1688/1688 [==============================] - ETA: 0s - loss: 7.7701e-05 - accuracy: 1.0000\n",
      "Epoch 13: val_accuracy did not improve from 1.00000\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 7.7701e-05 - accuracy: 1.0000 - val_loss: 4.9965e-04 - val_accuracy: 0.9998 - lr: 0.0100\n",
      "Learning Rate:  0.01\n",
      "Epoch 14/20\n",
      "1671/1688 [============================>.] - ETA: 0s - loss: 7.7634e-05 - accuracy: 1.0000\n",
      "Epoch 14: val_accuracy did not improve from 1.00000\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 7.7160e-05 - accuracy: 1.0000 - val_loss: 5.0216e-04 - val_accuracy: 0.9998 - lr: 0.0100\n",
      "Learning Rate:  0.01\n",
      "Epoch 15/20\n",
      "1688/1688 [==============================] - ETA: 0s - loss: 7.6624e-05 - accuracy: 1.0000\n",
      "Epoch 15: val_accuracy did not improve from 1.00000\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 7.6624e-05 - accuracy: 1.0000 - val_loss: 5.0382e-04 - val_accuracy: 0.9998 - lr: 0.0100\n",
      "Learning Rate:  0.01\n",
      "Epoch 16/20\n",
      "1685/1688 [============================>.] - ETA: 0s - loss: 7.6144e-05 - accuracy: 1.0000\n",
      "Epoch 16: val_accuracy did not improve from 1.00000\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 7.6109e-05 - accuracy: 1.0000 - val_loss: 5.0641e-04 - val_accuracy: 0.9998 - lr: 0.0100\n",
      "Learning Rate:  0.001\n",
      "Epoch 17/20\n",
      "1678/1688 [============================>.] - ETA: 0s - loss: 7.5383e-05 - accuracy: 1.0000\n",
      "Epoch 17: val_accuracy did not improve from 1.00000\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 7.5403e-05 - accuracy: 1.0000 - val_loss: 5.0663e-04 - val_accuracy: 0.9998 - lr: 0.0010\n",
      "Learning Rate:  0.001\n",
      "Epoch 18/20\n",
      "1680/1688 [============================>.] - ETA: 0s - loss: 7.5346e-05 - accuracy: 1.0000\n",
      "Epoch 18: val_accuracy did not improve from 1.00000\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 7.5353e-05 - accuracy: 1.0000 - val_loss: 5.0686e-04 - val_accuracy: 0.9998 - lr: 0.0010\n",
      "Learning Rate:  0.001\n",
      "Epoch 19/20\n",
      "1679/1688 [============================>.] - ETA: 0s - loss: 7.5278e-05 - accuracy: 1.0000\n",
      "Epoch 19: val_accuracy did not improve from 1.00000\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 7.5303e-05 - accuracy: 1.0000 - val_loss: 5.0708e-04 - val_accuracy: 0.9998 - lr: 0.0010\n",
      "Learning Rate:  0.001\n",
      "Epoch 20/20\n",
      "1684/1688 [============================>.] - ETA: 0s - loss: 7.5287e-05 - accuracy: 1.0000\n",
      "Epoch 20: val_accuracy did not improve from 1.00000\n",
      "1688/1688 [==============================] - 6s 3ms/step - loss: 7.5253e-05 - accuracy: 1.0000 - val_loss: 5.0730e-04 - val_accuracy: 0.9998 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x23602af33d0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x,train_y,batch_size=32,epochs=20,shuffle=True,validation_split=0.1,verbose=1,callbacks=[checkpoint,lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTIONAL API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras has two APIs for constructing models, the first is the Sequential API which we have used so far for simplicity sake, however, going forward we shall be using the more advanced functional API. The advantages might not seem obvious at this \n",
    "stage, but it is absolutely essential when designing more complex networks, as we shall do later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT needed libraries\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense,Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model\n",
    "def MiniModel(input_shape):\n",
    " images = Input(input_shape)\n",
    " net = Dense(units=128,activation=\"relu\")(images)\n",
    " net = Dense(units=128, activation=\"relu\")(net)\n",
    " net = Dense(units=128, activation=\"relu\")(net)\n",
    " net = Dense(units=10,activation=\"softmax\")(net)\n",
    " model = Model(inputs=images,outputs=net)\n",
    " return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 784)]             0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 128)               100480    \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134794 (526.54 KB)\n",
      "Trainable params: 134794 (526.54 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = MiniModel((784,))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the learning rate schedule function\n",
    "def lr_schedule(epoch):\n",
    "  lr = 0.1\n",
    "\n",
    "  if epoch > 15:\n",
    "    lr = lr/100\n",
    "\n",
    "  elif epoch > 10:\n",
    "    lr = lr/10\n",
    "\n",
    "  elif epoch >5:\n",
    "    lr = lr / 5\n",
    "\n",
    "  print ('Learning Rate: ', lr)\n",
    "\n",
    "  return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Learning Rate:  0.1\n"
     ]
    }
   ],
   "source": [
    "#Pass the scheduler function to the Learning Rate Scheduler class\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "\n",
    "#Directory in which to create models\n",
    "save_direc = os.path.join(os.getcwd(), 'mnistsavedmodels')\n",
    "\n",
    "\n",
    "#Name of model files\n",
    "model_name = 'mnistmodel.{epoch:03d}.h5'\n",
    "\n",
    "\n",
    "#Create Directory if it doesn't exist\n",
    "if not os.path.isdir(save_direc):\n",
    " os.makedirs(save_direc)\n",
    "\n",
    "\n",
    "#Join the directory with the model file\n",
    "modelpath = os.path.join(save_direc, model_name)\n",
    "checkpoint = ModelCheckpoint(filepath=modelpath,\n",
    " monitor='val_acc',\n",
    " verbose=1,\n",
    " period=1)\n",
    "\n",
    "\n",
    "#Specify the training components\n",
    "model.compile(optimizer=SGD(lr_schedule(0)),loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate:  0.1\n",
      "Epoch 1/20\n",
      "1667/1688 [============================>.] - ETA: 0s - loss: 0.2926 - accuracy: 0.9105\n",
      "Epoch 1: saving model to c:\\Users\\PC\\Desktop\\deep_learning\\mnistsavedmodels\\mnistmodel.001.h5\n",
      "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2909 - accuracy: 0.9110 - val_loss: 0.1474 - val_accuracy: 0.9562 - lr: 0.1000\n",
      "Learning Rate:  0.1\n",
      "Epoch 2/20\n",
      "  29/1688 [..............................] - ETA: 6s - loss: 0.1343 - accuracy: 0.9558"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\envs\\comp_vision\\Lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1685/1688 [============================>.] - ETA: 0s - loss: 0.1188 - accuracy: 0.9632\n",
      "Epoch 2: saving model to c:\\Users\\PC\\Desktop\\deep_learning\\mnistsavedmodels\\mnistmodel.002.h5\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.1187 - accuracy: 0.9632 - val_loss: 0.0912 - val_accuracy: 0.9713 - lr: 0.1000\n",
      "Learning Rate:  0.1\n",
      "Epoch 3/20\n",
      "1669/1688 [============================>.] - ETA: 0s - loss: 0.0812 - accuracy: 0.9745\n",
      "Epoch 3: saving model to c:\\Users\\PC\\Desktop\\deep_learning\\mnistsavedmodels\\mnistmodel.003.h5\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.0815 - accuracy: 0.9744 - val_loss: 0.0834 - val_accuracy: 0.9748 - lr: 0.1000\n",
      "Learning Rate:  0.1\n",
      "Epoch 4/20\n",
      "1680/1688 [============================>.] - ETA: 0s - loss: 0.0642 - accuracy: 0.9797\n",
      "Epoch 4: saving model to c:\\Users\\PC\\Desktop\\deep_learning\\mnistsavedmodels\\mnistmodel.004.h5\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.0645 - accuracy: 0.9796 - val_loss: 0.0776 - val_accuracy: 0.9775 - lr: 0.1000\n",
      "Learning Rate:  0.1\n",
      "Epoch 5/20\n",
      "1668/1688 [============================>.] - ETA: 0s - loss: 0.0480 - accuracy: 0.9852\n",
      "Epoch 5: saving model to c:\\Users\\PC\\Desktop\\deep_learning\\mnistsavedmodels\\mnistmodel.005.h5\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.0481 - accuracy: 0.9852 - val_loss: 0.0740 - val_accuracy: 0.9792 - lr: 0.1000\n",
      "Learning Rate:  0.1\n",
      "Epoch 6/20\n",
      "1670/1688 [============================>.] - ETA: 0s - loss: 0.0412 - accuracy: 0.9867\n",
      "Epoch 6: saving model to c:\\Users\\PC\\Desktop\\deep_learning\\mnistsavedmodels\\mnistmodel.006.h5\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.0412 - accuracy: 0.9867 - val_loss: 0.0680 - val_accuracy: 0.9813 - lr: 0.1000\n",
      "Learning Rate:  0.02\n",
      "Epoch 7/20\n",
      "1685/1688 [============================>.] - ETA: 0s - loss: 0.0166 - accuracy: 0.9953\n",
      "Epoch 7: saving model to c:\\Users\\PC\\Desktop\\deep_learning\\mnistsavedmodels\\mnistmodel.007.h5\n",
      "1688/1688 [==============================] - 6s 3ms/step - loss: 0.0166 - accuracy: 0.9953 - val_loss: 0.0602 - val_accuracy: 0.9845 - lr: 0.0200\n",
      "Learning Rate:  0.02\n",
      "Epoch 8/20\n",
      "1685/1688 [============================>.] - ETA: 0s - loss: 0.0118 - accuracy: 0.9973\n",
      "Epoch 8: saving model to c:\\Users\\PC\\Desktop\\deep_learning\\mnistsavedmodels\\mnistmodel.008.h5\n",
      "1688/1688 [==============================] - 6s 3ms/step - loss: 0.0118 - accuracy: 0.9973 - val_loss: 0.0608 - val_accuracy: 0.9847 - lr: 0.0200\n",
      "Learning Rate:  0.02\n",
      "Epoch 9/20\n",
      "1686/1688 [============================>.] - ETA: 0s - loss: 0.0098 - accuracy: 0.9980\n",
      "Epoch 9: saving model to c:\\Users\\PC\\Desktop\\deep_learning\\mnistsavedmodels\\mnistmodel.009.h5\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.0098 - accuracy: 0.9980 - val_loss: 0.0632 - val_accuracy: 0.9845 - lr: 0.0200\n",
      "Learning Rate:  0.02\n",
      "Epoch 10/20\n",
      "1685/1688 [============================>.] - ETA: 0s - loss: 0.0084 - accuracy: 0.9984\n",
      "Epoch 10: saving model to c:\\Users\\PC\\Desktop\\deep_learning\\mnistsavedmodels\\mnistmodel.010.h5\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.0084 - accuracy: 0.9984 - val_loss: 0.0645 - val_accuracy: 0.9850 - lr: 0.0200\n",
      "Learning Rate:  0.02\n",
      "Epoch 11/20\n",
      "1664/1688 [============================>.] - ETA: 0s - loss: 0.0074 - accuracy: 0.9987\n",
      "Epoch 11: saving model to c:\\Users\\PC\\Desktop\\deep_learning\\mnistsavedmodels\\mnistmodel.011.h5\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.0074 - accuracy: 0.9987 - val_loss: 0.0642 - val_accuracy: 0.9850 - lr: 0.0200\n",
      "Learning Rate:  0.01\n",
      "Epoch 12/20\n",
      "1683/1688 [============================>.] - ETA: 0s - loss: 0.0061 - accuracy: 0.9990\n",
      "Epoch 12: saving model to c:\\Users\\PC\\Desktop\\deep_learning\\mnistsavedmodels\\mnistmodel.012.h5\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.0061 - accuracy: 0.9990 - val_loss: 0.0647 - val_accuracy: 0.9857 - lr: 0.0100\n",
      "Learning Rate:  0.01\n",
      "Epoch 13/20\n",
      "1675/1688 [============================>.] - ETA: 0s - loss: 0.0056 - accuracy: 0.9990\n",
      "Epoch 13: saving model to c:\\Users\\PC\\Desktop\\deep_learning\\mnistsavedmodels\\mnistmodel.013.h5\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.0056 - accuracy: 0.9990 - val_loss: 0.0658 - val_accuracy: 0.9847 - lr: 0.0100\n",
      "Learning Rate:  0.01\n",
      "Epoch 14/20\n",
      "1680/1688 [============================>.] - ETA: 0s - loss: 0.0053 - accuracy: 0.9992\n",
      "Epoch 14: saving model to c:\\Users\\PC\\Desktop\\deep_learning\\mnistsavedmodels\\mnistmodel.014.h5\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.0053 - accuracy: 0.9992 - val_loss: 0.0667 - val_accuracy: 0.9847 - lr: 0.0100\n",
      "Learning Rate:  0.01\n",
      "Epoch 15/20\n",
      "1669/1688 [============================>.] - ETA: 0s - loss: 0.0050 - accuracy: 0.9993\n",
      "Epoch 15: saving model to c:\\Users\\PC\\Desktop\\deep_learning\\mnistsavedmodels\\mnistmodel.015.h5\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.0049 - accuracy: 0.9993 - val_loss: 0.0673 - val_accuracy: 0.9853 - lr: 0.0100\n",
      "Learning Rate:  0.01\n",
      "Epoch 16/20\n",
      "1676/1688 [============================>.] - ETA: 0s - loss: 0.0047 - accuracy: 0.9994\n",
      "Epoch 16: saving model to c:\\Users\\PC\\Desktop\\deep_learning\\mnistsavedmodels\\mnistmodel.016.h5\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.0047 - accuracy: 0.9994 - val_loss: 0.0682 - val_accuracy: 0.9848 - lr: 0.0100\n",
      "Learning Rate:  0.001\n",
      "Epoch 17/20\n",
      "1687/1688 [============================>.] - ETA: 0s - loss: 0.0042 - accuracy: 0.9995\n",
      "Epoch 17: saving model to c:\\Users\\PC\\Desktop\\deep_learning\\mnistsavedmodels\\mnistmodel.017.h5\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.0042 - accuracy: 0.9995 - val_loss: 0.0679 - val_accuracy: 0.9845 - lr: 0.0010\n",
      "Learning Rate:  0.001\n",
      "Epoch 18/20\n",
      "1676/1688 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 0.9996\n",
      "Epoch 18: saving model to c:\\Users\\PC\\Desktop\\deep_learning\\mnistsavedmodels\\mnistmodel.018.h5\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.0041 - accuracy: 0.9996 - val_loss: 0.0679 - val_accuracy: 0.9847 - lr: 0.0010\n",
      "Learning Rate:  0.001\n",
      "Epoch 19/20\n",
      "1667/1688 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 0.9996\n",
      "Epoch 19: saving model to c:\\Users\\PC\\Desktop\\deep_learning\\mnistsavedmodels\\mnistmodel.019.h5\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.0041 - accuracy: 0.9996 - val_loss: 0.0679 - val_accuracy: 0.9845 - lr: 0.0010\n",
      "Learning Rate:  0.001\n",
      "Epoch 20/20\n",
      "1681/1688 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 0.9996\n",
      "Epoch 20: saving model to c:\\Users\\PC\\Desktop\\deep_learning\\mnistsavedmodels\\mnistmodel.020.h5\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.0041 - accuracy: 0.9996 - val_loss: 0.0679 - val_accuracy: 0.9847 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x23626e30f10>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x,train_y,batch_size=32,epochs=20,shuffle=True,validation_split=0.1,verbose=1,callbacks=[checkpoint,lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 7ms/step - loss: 0.0617 - accuracy: 0.9827\n",
      "Accuracy:  0.982699990272522\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the accuracy of the test dataset\n",
    "accuracy = model.evaluate(x=test_x,y=test_y,batch_size=32)\n",
    "print(\"Accuracy: \",accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp_vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
